# Hybrid Language Identifier

A fast, interpretable, and highly accurate language identification system that combines classic n-gram analysis with modern subword tokenization. This project provides a resource-efficient alternative to large language models for the specific task of language identification, making it ideal for high-volume NLP pipelines.

## Methodology

The system operates on a profile-matching principle. For each language, a "profile" is generated, which is a frequency-ordered list of its most common textual features. An input text is identified by generating its own profile and calculating a **Rank Distance** score against the pre-built language profiles to find the closest match.

Two models are implemented:

1. **Simple Model (N-gram based):** This model analyzes the "surface texture" of text using character n-grams. It is extremely fast and robust to typos and noise but lacks semantic understanding and can struggle to differentiate between closely related languages.

2. **Advanced Hybrid Model:** This model provides a more accurate and resilient analysis by combining two signals:
  - **Character N-grams:** The same robust signal as the simple model.
  - **Subword Tokens:** Generated by a pre-trained multilingual tokenizer (`bert-base-multilingual-cased`), this signal provides deeper grammatical and semantic context.

The hybrid approach uses the n-gram model as a safety net for noisy text while leveraging the subword model's structural understanding, resulting in a system that is more accurate than either method alone.

## Implementation

The project is structured as a modular Python package with several key components:

- **Profile Generation:** The `build_profiles.py` script uses the Hugging Face `datasets` library to download text corpora from the `wikiann` dataset. It then processes this text to generate and save the `.json` language profiles required by the identifiers. Although the pre-created profiles are already present in `profiles`, you can clear this out if you choose to create fresh profiles.

- **Command-Line Interface (CLI):** The `main.py` script provides a user-friendly CLI built with `click`. It allows for direct interaction with both the simple and advanced models from the terminal.

- **Web API:** The `api.py` script launches a high-performance REST API using `FastAPI`. This exposes the identification models over HTTP, allowing for easy integration into other applications and services.

## Quickstart

1. **Clone and Install Dependencies:**

```bash
git clone https://github.com/your-username/language-identifier.git
cd language-identifier
pip install -r requirements.txt
```

2. **Build Profiles (One-time setup):**

```bash
python build_profiles.py
```

3. **Run the System:**

  - **Via CLI:**
  
    ```bash
    # Use the superior hybrid model (default)
    python main.py "This is a test sentence."
    ```

  - **Via Web API:**
  
    ```bash
    uvicorn api:app --reload
    ```

  The interactive API documentation will be available at `http://127.0.0.1:8000/docs`.

## Testing

The project includes a full test suite using `pytest`. To run the tests, execute the following command from the project root:

```bash
pytest
```

# Licence
This project is licensed under the **MIT License**.